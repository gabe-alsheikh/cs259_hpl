First change: Moved writeBuffer and readBuffer out of the iterative loop for 
the LU decomposition kernel call.

lu259_cl_ITERKERNEL_v1.X
This iteration involved a kernel with N-n-1 work groups and an iterative scheme to
perform the LU decomposition. Each work group is a single item. Substitution is
done in the host file.
SUCCESSFULLY TESTED ON FPGA USING FLOATS

lu259_cl_ITERKERNEL_v2.X
Converted the previous kernel into a blocked scheme, with each work group having
N/BLOCK_SIZE items (global size is (N-n-1)*N/BLOCK_SIZE)
No speedup over v1, but it could potentially provide it with HLS optimizations.

lu259_cl_ITERKERNEL_v3.X
Changed the kernel to process denom and nextDenom each time and added in pivoting in
the host.

lu259_cl_ITERKERNEL_v4.X
Added in kernels for forward substitution and backward substitution

lu259_cl_ITERKERNEL_v5.c
Added in a "fix" for the less of precision with floats during the check.

lu259_clITERKERNEL_v5.cl and .h
Added in Optimization #1 so that there is no multiplication in the address offset calculations in the arrays.

lu259_cl_ITERKERNEL_v6.c
Put clCreateKernel stuff before the time starts and made Block size to 128

lu259_cl_ITERKERNEL_v6.cl and .h
Changed block size to 128

lu259_cl_ITERKERNEL_v7.c 
Removed unused variables being allocated with memory

lu259_cl_ITERKERNEL_v7.cl
Added a pipeline attribute before the second loop in LUFact

lu259_cl_ITERKERNEL_v8.c
Removed substitution kernel calls and initialization

lu259_cl_ITERKERNEL_v8.cl and h
Removed substitution kernels

lu259_cl_ITERKERNEL_v9.c
Changes to local/global work size in host and cStart in kernel
Changed group sizes to new scheme:
Basically i'm trying to have the first w-i include column n (though technically we just need n+1), since we can ignore the stuff to the left of it
the last slice should line up with the end
change the host code to 1D w-g's and 1D w-i's, usual number of groups, and the given number of w-i's (this changes every BLOCK_SIZE iterations)
before:
__________
|_________|
|         |
|_________|
after:
_________
|  ______|
| |      |
|_|______|
*|__|____|
the implication is that we don't have work items that do nothing
(actually, there's an OB1 where one w-i each group is doing nothing occasionally, but it's not a big deal)
basically, before it made w-i's for the area under the line inside the matrix
after, it deals with the inner box only, which is where the action is

lu259_cl_ITERKERNEL_v9.cl and h
Changed cStart to reflect new change


lu259_cl_ITERKERNEL_v9b.X
Changed to a 1D blocked scheme that works off blocks in the matrix rather than slices of rows

lu259_cl_ITERKERNEL_v9c.X
Uses 2D block scheme and block size changed from 16 to 32 in order to facilitate N=1024 on FPGA
(was killed before for 1024, probably because global size was too large, not large enough BLOCK_SIZE to divide)

lu259_cl_ITERKERNEL_v9d.X
Removes the user of nextDenom from the host file and kernel


II. OPTIMIZATIONS:
OPTIMIZATIONS DURING DEVELOPMENT:
-Our in-place A matrix is a minor optimization, but since that was our "starting point" we don't have numbers for it.
-Our work groups are functions of N and the BLOCK_SIZE, rather than having work-groups of size 1.
-We observed we did not have to have a read/write for A on every iteration of the loop. Only needing to read it when it's 
needed back in host memory.
-Moved writeBuffer and readBuffer out of the iterative loop for the LU decomposition kernel call.



1) (corresponds to v5)
I declared two variables, rStart and nStart, that make it so that there is no multiplication in the
address offset calculations. Done in the LUFact kernel.

2) (corresponds to v6)
Changing block size to 128. Very minimal speedups.

3) 
Pipelining the second loop in LUFact kernel

4)
-N 4 duplication for LUFact kernel (change in lu259_fpga.tcl; use v7)

5)
REMOVED SUBSTITUTION KERNELS (corresponds to v8)
Why? The biggest thing is that it does 2048 kernel launches for O(N) operations for each substitution
For LUFact, it does 1024 launches for O(N^2) operations
Sub takes 1/512ish of computation time
This is part of the reason why i/we tried a single kernel first
This is done also to allow for more area for the the LU factorization kernel

6) (corresponds to v9/v9b/v9c/v9d)
Changed group sizes to new scheme:
Basically i'm trying to have the first w-i include column n (though technically we just need n+1), since we can ignore the stuff to the left of it
the last slice should line up with the end
change the host code to 1D w-g's and 1D w-i's, usual number of groups, and the given number of w-i's (this changes every BLOCK_SIZE iterations)
THIS IS CURRENTLY ON HOLD! IT IS NOT WORKING CORRECTLY ON THE FPGA AND EXPERIENCING A SLOWDOWN
HOLD RELEASED. VERSION 9B WORKING ON FPGA, BUT PROCESS KILLED FOR N 1024
SAME WITH VERSION 9C.
BLOCK SIZE INCREASED TO 32! PROCESS DOES NOT DIE AT N 1024. DEATH WAS DUE TO OVERLY LARGE GLOBAL SIZE

7) (corresponds to v10) 
Used local memory in kernel for current rows of L and A

RUN TIMES ON FPGA:
0) No optimizations, base version with A modified in-place, both BLOCK_SIZE are 64
N		Time(s)
64		0.16
128		0.85
256		5.19
512		35.02
1024	254.29

1) Optimization #1
N		Time(s)
64		0.16
128		0.83
256		5.07
512		34.16
1024	247.58


2) Optimization #2
N		Time(s)
64		N/A; can't do since block size is 128
128		0.83
256		5.04
512		33.89
1024	245.25
Why not a huge speedup?
Guess: Since there's no local memory, block size will not affect it much


3) Optimization #3
N		Time(s)
64		N/A
128		0.83
256		5.01
512		33.64
1024	243.39

4) Optimization #4
N		Time(s)
64		N/A
128		0.42
256		1.87
512		12.27
1024	94.53

5) Optimization #5
N		Time(s)
64		N/A
128		0.39
256		1.72
512		11.87
1024	93.36

6) Optimization #6
NOTE: BLOCK SIZE 32
N		Time(s)
64		0.03
128		0.20
256		1.54
512		11.89
1024	94.90

7) Optimization #7
NOTE: BLOCK SIZE 32
N		Time(s)
64		0.02
128		0.12
256		0.88
512		6.81
1024	54.15
